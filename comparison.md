
# Comparisons with our Model
ðŸ§  1. Philosophical and Functional Shift

Feature | Original Rule-Agent | Neuro Symbolic Approach
--- | --- | ---
Philosophy | Rule-based symbolic reasoning aided by LLM-generated structured inputs | Full integration of symbolic ontology, logical reasoning, machine learning, and generative LLMs
Goal | Translate NL to parameters for rule services (ODM) | Ground LLM interpretation in ontologies, reason over logic, and generate explanations
Reasoning | Delegated to ODM (external decision service) | In-graph ontology reasoning via Owlready2 + reasoning + contradiction detection
Interpretability | Output mapped back to rules and decisions | Natural Language + Logical Form + Explanation

ðŸ§© 2. Ontology and Symbolic Reasoning

Category | Original Rule-Agent | Neuro Symbolic Approach
--- | --- | ---
Ontology Use | Uses precompiled .jar ruleapp (opaque symbolic logic) | OWL ontologies dynamically defined, queried, and reasoned over using owlready2
Reasoner | External: IBM ODM | Internal: HermiT via owlready2â€™s sync_reasoner()
Evaluation | No direct logical validation | validate truth/falsehood of logical forms in ontology with reasoned feedback
Relationships | Preset rules (.brl) | Declarative OWL object properties (CausesFailure, etc.)

ðŸ§¬ 3. Learning and Generalization

Category | Original Rule-Agent | Neuro Symbolic Approach
--- | --- | ---
LLM Role | Converts queries into input JSON for rules | Synthesizes statements, verifies ontological truth, rewrites wrong statements
Data Generation | Static, hardcoded examples | Automatic training set generation from ontology relationships (true & false)
Model Training | No learning model | Trains logistic regression on NL â†’ logical form using generated training pairs

ðŸ“¤ 4. Pipeline Architecture Comparison

Original Pipeline:

- Natural Language â†’ Prompt Template (LLM) â†’ Structured JSON â†’ Rule Service â†’ Output

Neuro Symbolic Approach Pipeline:

- Natural Language â†’
- ML Classification â†’ Logical Statement â†’
- Ontology Reasoning (truth/falsehood) â†’
- Prompt to LLM with Context â†’
- Natural Language Answer + Logical Form + Explanation

ðŸ§ª 5. Explanation and Contradiction Detection

Capability | Original Rule-Agent | Neuro Symbolic Approach
--- | --- | ---
Contradiction? | Not handled | Explicitly identifies logical contradictions via class mismatch or disjoint types
Explanations | Human-generated | Generated using parsed ontology structure
Negative Cases | Not modeled | Actively generates and trains on true + false variations

ðŸ“ˆ 6. ML Training Dataset from Ontology

The Neuro Symbolic Approach dynamically:

- Generates (subject.property(object)) logical forms
- Evaluates true/false from ontology
- Converts to natural language (verbalizer)
- Trains ML classifier to map back from NL â†’ logic

This is a tight neuro-symbolic loop: logic generates training data â†’ ML learns mappings â†’ ML produces logic â†’ logic gets validated.

ðŸ§  7. Model Interpretation and Generalization

Feature | Original Rule-Agent | Neuro Symbolic Approach
--- | --- | ---
Handles Uncertainty | LLM default behavior | Explicit handling with reasoned rejections, logical consistency
Ontology-Driven Response | No | Yes â€“ ontology forms the epistemic foundation
Factual Grounding | Delegated to external service | Grounded in internal ontology with reasoner

ðŸ›  Key Implementation Innovations

- Ontology-driven training data generator
- Verbalizer functions with rich linguistic variation
- Logical consistency checking with explanations
- LLM synthesis prompt informed by ontology, truth table, evaluation result

ðŸ§© Core Advantages of the Neuro Symbolic Approach

1. Interpretable â€” Logical forms, true/false evidence, explanations  
2. Trainable â€” Uses symbolic facts to generate ML data  
3. Robust â€” Detects contradictions, supports false inputs  
4. Scalable â€” Extendable with new OWL ontologies  
5. Unified â€” Neuro and symbolic layers interact bidirectionally  

